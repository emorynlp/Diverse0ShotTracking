
from dextrous.launch import launch


machine = 'h100'

opt_args = dict(
    h100=dict(
        gen_batch_size=6,
        gradient_accumulation_steps=128,
    ),
    tebuna=dict(
        gen_batch_size=3,
        gradient_accumulation_steps=256,
    ),
)

for groupcode, domain, base in [
    # ('restaurant', 'ex/LlamaTracker/SupernovaOssus/7'),
    # ('attraction', 'ex/LlamaTracker/SpiritedMon/8'),
    # ('train', 'ex/LlamaTracker/SpiritedPamarthe/9'),
    # ('hotel', 'ex/LlamaTracker/UntamedCatoNeimoidia/11'),
    # ('taxi', 'ex/LlamaTracker/HeroicLandosFarm/6'),
    # ('restaurant', 'ex/LlamaTracker/DazzlingDengar/21'),
    # ('attraction', 'ex/LlamaTracker/DazzlingDengar/21'),
    # ('train', 'ex/LlamaTracker/DazzlingDengar/21'),
    # ('hotel', 'ex/LlamaTracker/DazzlingDengar/21'),
    # ('taxi', 'ex/LlamaTracker/DazzlingDengar/21'),
    # ('restaurant', 'ex/LlamaTracker/UnforgettableGreedo/4'),
    # ('attraction', 'ex/LlamaTracker/EnthrallingScarif/5'),
    # ('train', 'ex/LlamaTracker/IllustriousGreedo/9'),
    # ('hotel', 'ex/LlamaTracker/LuminousLuke/10'),
    # ('taxi', 'ex/LlamaTracker/PulsarAqualish/4'),
    # ('llama-dot-fientune-mwoz-icl', 'restaurant', 'ex/LlamaTracker/SereneBaze/8'),
    # ('llama-dot-fientune-mwoz-icl','attraction', 'ex/LlamaTracker/BoldBB9E/10'),
    # ('llama-dot-fientune-mwoz-icl','train', 'ex/LlamaTracker/UnchartedTatooII/6'),
    # ('llama-dot-fientune-mwoz-icl','hotel', 'ex/LlamaTracker/UnchartedPaz/11'),
    # ('llama-dot-fientune-mwoz-icl','taxi', 'ex/LlamaTracker/VibrantTaris/4'),
    ('llama-dot-baseline-mwoz-icl', 'hotel', 'ex/LlamaTracker/DazzlingAhsoka/10'),
    ('llama-dot-baseline-mwoz-icl', 'restaurant', 'ex/LlamaTracker/AstralPorg/7'),
    ('llama-dot-baseline-mwoz-icl', 'attraction', 'ex/LlamaTracker/ThunderousIlum/10'),
    ('llama-dot-baseline-mwoz-icl', 'train', 'ex/LlamaTracker/ResplendentMace/8'),
    ('llama-dot-baseline-mwoz-icl', 'taxi', 'ex/LlamaTracker/RogueNevarro/8'),

]:
    launch.LlamaLaunch(
        groupcode=groupcode,
        base=base,
        approach='LlamaTracker',
        param_magnitude='13b',
        train_data='data/mwoz2.4_3s/train',
        eval_data='data/mwoz2.1_3s/test',
        train_downsample=None,
        eval_dialogue_downsample=None,
        eval_exclude_speakers='bot',
        test_domains=[domain],
        eval_all_slots_per_domain=True,
        prediction_lowercase=True,
        prediction_fuzzy_match_candidates=True,
        epochs=0,
        max_sequence_length=1024,
        train_batch_size=256,
        gradient_accumulation_steps=opt_args[machine]['gradient_accumulation_steps'],
        warmup_steps=100,
        learning_rate=5e-5,
        weight_decay=0.0,
        quantize='nf4',
        lora_merge_on_load=False,
        lora=32,
        lora_alpha=64,
        lora_dropout=0.0,
        train_all_slots_per_domain=True,
        neg_examples_ratio=0.0,
        exclude_speakers=['bot'],
        train_prop_add_continuation=1.0,
        train_percent_with_description=1.0,
        train_percent_description_only=0.0,
        train_percent_with_categories=0.0,
        train_percent_with_value_exs=0.0,
        train_percent_value_ex_includes_actual_value=None,
        train_remove_request_token_percent=None,
        train_filters_out_descriptions_with_actual_value=None,
        eval_with_categories=False,
        eval_with_value_exs=False,
        uncased=False,
        num_beams=3,
        repetition_penalty=1.0,
        gen_batch_size=opt_args[machine]['gen_batch_size'],
        max_output_length=16,
        rng_seed=21,
        do_eval_after_all_training=True,
        calculate_eval_perplexity=False,
        yield_every_x_epochs=0.10,
        dynamic_tokenization=True,
        notifications=True,
    )
